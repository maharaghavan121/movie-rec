# Movie Recommender Refactoring Plan

## Overview

Restructure the movie recommender codebase to cleanly separate training and serving, with a production-ready API. This plan eliminates the `hello.py` dependency and establishes clear boundaries between offline training and online serving.

---

## Goals

1. **Clean separation**: Training script vs. serving API
2. **Shared core logic**: Data loading and matrix building in one place
3. **Production-ready API**: FastAPI with both known-user and cold-start endpoints
4. **Clear transpose handling**: Transpose once at training, never at serving
5. **Artifact-based serving**: API loads pre-trained model, no training dependencies

---

## Final Structure

```
movie-recs/
├── README.md                  # Updated with API usage
├── requirements.txt           # Dependencies
├── PLAN.md                    # This file
│
├── data_small/                # MovieLens data (user downloads, .gitignored)
│   ├── movies.csv
│   ├── ratings.csv
│   ├── links.csv
│   └── tags.csv
│
├── artifacts/                 # Generated by training (.gitignored except .gitkeep)
│   ├── .gitkeep
│   ├── als.npz                # User and item factors
│   ├── users.npy              # User ID array
│   ├── items.npy              # Movie ID array
│   ├── X_users_items.npz      # Sparse serving matrix (users × items)
│   └── movie_map.csv          # Movie ID → title mapping
│
├── app/                       # API package
│   ├── __init__.py            # Makes app a package (can be empty)
│   ├── core.py                # Shared data/matrix functions
│   ├── recommender.py         # Serving layer (loads artifacts)
│   └── main.py                # FastAPI app
│
└── train_als.py               # Training script (imports from app.core)
```

---

## Implementation Steps

### **Step 1: Create Package Structure (5 min)**

```bash
# From repo root
mkdir -p app
touch app/__init__.py
mkdir -p artifacts
touch artifacts/.gitkeep
```

### **Step 2: Create `app/core.py` (10 min)**

**Purpose**: Shared functions for data loading, splitting, indexing, and matrix building.

**Key functions**:
- `load_data()` - Load MovieLens CSVs
- `make_splits(ratings, thresh=4)` - Time-based train/val/test split
- `build_index_maps(ratings, train_data)` - Create user/movie ID mappings
- `train_matrix(train_data, user_ids, movie_ids, user_index_map, movie_index_map, alpha=20.0)` - Build sparse users×items matrix

**Important details**:
- Matrix orientation: **users × items** (this is the serving orientation)
- Use `.to_numpy()` instead of `.values` (pandas best practice)
- Return sparse CSR matrix for efficient row slicing

**File location**: `app/core.py`

**Content**: See detailed code below in "Code Changes" section.

---

### **Step 3: Update `train_als.py` (15 min)**

**Purpose**: One-off training script that saves artifacts for serving.

**Changes**:
1. **Import from app.core**: `from app.core import load_data, make_splits, build_index_maps, train_matrix`
2. **Transpose once at training**: `als.fit(X_users_items.T)` (implicit expects items×users)
3. **Save artifacts**:
   - `als.npz` - user_factors, item_factors (float32)
   - `users.npy` - User ID array
   - `items.npy` - Movie ID array
   - `X_users_items.npz` - Serving matrix (users×items, CSR format)
   - `movie_map.csv` - Movie ID → title mapping
4. **Add logging**: Print progress at each stage
5. **Add evaluation**: Compute Recall@10 on test set

**Key principle**: Train on transpose, save serving matrix untransposed.

**File location**: `train_als.py` (overwrite existing)

**Content**: See detailed code below.

---

### **Step 4: Create `app/recommender.py` (20 min)**

**Purpose**: Serving layer that loads artifacts and provides recommendation methods.

**Class**: `ALSRecommender`

**Methods**:
- `__init__()` - Load all artifacts, validate they exist
- `recommend_known(user_id: int, k: int = 10)` - Recommendations for existing user
- `recommend_cold(liked_movie_ids: list, k: int = 10)` - Cold-start recommendations

**Key implementation details**:

1. **Error handling**: Check if artifacts exist, raise helpful errors if not
2. **Known-user path**:
   - Lookup user index in `self.uidx`
   - Extract user row from `self.X_users_items[u:u+1]`
   - Call `model.recommend(u, user_row, N=k, filter_already_liked_items=True)`
3. **Cold-start path**:
   - Build synthetic sparse row from `liked_movie_ids`
   - Use confidence weight 21.0 (1 + alpha)
   - Call `model.recommend(0, synthetic_row, N=k, recalculate_user=True)`
   - Manually filter seed movies from results

**File location**: `app/recommender.py`

**Content**: See detailed code below.

---

### **Step 5: Create `app/main.py` (10 min)**

**Purpose**: FastAPI endpoints for serving recommendations.

**Endpoints**:

1. **GET /health** - Health check
2. **GET /recommend** - Main recommendation endpoint
   - Query params:
     - `user_id` (optional): Known user ID
     - `liked` (optional): Comma-separated movie IDs for cold-start
     - `k` (default: 10): Number of recommendations
   - Logic:
     - If `user_id` provided and exists → known-user path
     - Otherwise → cold-start path with `liked` as seed

**File location**: `app/main.py`

**Content**: See detailed code below.

---

### **Step 6: Update `.gitignore` (2 min)**

**Purpose**: Don't track generated artifacts or data.

**Add these lines**:
```gitignore
# Data (user downloads)
data_small/

# Generated artifacts
artifacts/*
!artifacts/.gitkeep

# Python
__pycache__/
*.pyc
.venv/
*.npz
*.npy
```

**File location**: `.gitignore`

---

### **Step 7: Update `requirements.txt` (2 min)**

**Purpose**: Ensure all dependencies are listed.

**Required packages**:
```
numpy
pandas
scipy
implicit==0.7.2
fastapi
uvicorn[standard]
```

**File location**: `requirements.txt`

---

### **Step 8: Update README.md (15 min)**

**Purpose**: Document new structure and API usage.

**Add these sections**:

1. **Project Structure** - Show directory tree
2. **Quick Start** - Setup, train, run API
3. **API Usage** - Example curl commands
4. **Development** - How to run locally

**File location**: `README.md` (append to existing)

**Content**: See detailed addition below.

---

## Code Changes

### **File: `app/__init__.py`**

```python
# Empty file - makes app a package
```

---

### **File: `app/core.py`**

```python
"""
Shared data loading and matrix building functions.
Used by both training (train_als.py) and serving (app/recommender.py).
"""
import numpy as np
import pandas as pd
import scipy.sparse as sp


def load_data():
    """Load MovieLens CSV files."""
    movies = pd.read_csv("data_small/movies.csv")
    ratings = pd.read_csv("data_small/ratings.csv")
    links = pd.read_csv("data_small/links.csv")
    tags = pd.read_csv("data_small/tags.csv")
    return movies, ratings, links, tags


def make_splits(ratings, thresh=4):
    """
    Time-based train/val/test split.

    For each user with ≥2 positive ratings (rating ≥ thresh):
    - Test: last positive rating
    - Val: second-to-last positive rating
    - Train: all earlier positive ratings

    Args:
        ratings: DataFrame with userId, movieId, rating, timestamp
        thresh: Minimum rating to consider positive (default: 4.0)

    Returns:
        train, val, test DataFrames
    """
    df = ratings.sort_values(['userId', 'timestamp']).copy()
    df['is_positive_rating'] = df["rating"] >= thresh

    pos = df[df["is_positive_rating"]].copy()
    pos["rnk"] = pos.groupby("userId")["timestamp"].rank(method="first")
    pos["num_positives"] = pos.groupby("userId")["userId"].transform("size")
    pos = pos[pos["num_positives"] >= 2].copy()

    test = pos[pos["rnk"] == pos["num_positives"]].copy()
    val = pos[pos["rnk"] == pos["num_positives"] - 1].copy()
    train = pos[pos["rnk"] < pos["num_positives"] - 1].copy()

    return train, val, test


def build_index_maps(ratings, train_data):
    """
    Build user and movie ID to index mappings.

    Args:
        ratings: Full ratings DataFrame (unused, kept for compatibility)
        train_data: Training DataFrame

    Returns:
        user_ids: Sorted array of user IDs
        movie_ids: Sorted array of movie IDs
        user_index_map: Dict mapping user_id → index
        movie_index_map: Dict mapping movie_id → index
    """
    user_ids = np.sort(train_data["userId"].unique())
    movie_ids = np.sort(train_data["movieId"].unique())
    user_index_map = {u: i for i, u in enumerate(user_ids)}
    movie_index_map = {m: i for i, m in enumerate(movie_ids)}
    return user_ids, movie_ids, user_index_map, movie_index_map


def train_matrix(train_data, user_ids, movie_ids, user_index_map, movie_index_map, alpha=20.0):
    """
    Build sparse user-item interaction matrix.

    Matrix orientation: users × items (serving orientation)
    Confidence weighting: 1 + alpha for positive interactions

    Args:
        train_data: Training DataFrame with userId, movieId
        user_ids: Array of user IDs
        movie_ids: Array of movie IDs
        user_index_map: user_id → index mapping
        movie_index_map: movie_id → index mapping
        alpha: Confidence parameter (default: 20.0)

    Returns:
        Sparse CSR matrix of shape (n_users, n_items)
    """
    rows = train_data["userId"].map(user_index_map).to_numpy()
    cols = train_data["movieId"].map(movie_index_map).to_numpy()
    data = np.full(len(train_data), 1.0 + alpha, dtype="float32")

    X = sp.coo_matrix(
        (data, (rows, cols)),
        shape=(len(user_ids), len(movie_ids)),
        dtype="float32"
    ).tocsr()

    return X
```

---

### **File: `train_als.py`**

```python
"""
Train ALS model and save artifacts for serving.

Usage:
    python train_als.py

Outputs:
    artifacts/als.npz - User and item factors
    artifacts/users.npy - User ID array
    artifacts/items.npy - Movie ID array
    artifacts/X_users_items.npz - Serving matrix (users × items)
    artifacts/movie_map.csv - Movie titles
"""
import numpy as np
import scipy.sparse as sp
from pathlib import Path
from implicit.als import AlternatingLeastSquares

from app.core import load_data, make_splits, build_index_maps, train_matrix

# Artifact directory
ART = Path("artifacts")
ART.mkdir(exist_ok=True, parents=True)


def simple_recall_at_k(als, X, user_ids, movie_ids, user_index_map, test_data, k=10):
    """Compute Recall@K on test set."""
    hits = total = 0
    for _, row in test_data.iterrows():
        u_id, m_id = row["userId"], row["movieId"]
        if u_id not in user_index_map:
            continue

        u_idx = user_index_map[u_id]
        user_row = X[u_idx:u_idx+1]
        recs = als.recommend(u_idx, user_row, N=k, filter_already_liked_items=True)
        pred_movies = {int(movie_ids[i]) for i, _ in recs}

        if m_id in pred_movies:
            hits += 1
        total += 1

    return hits / max(total, 1)


def main():
    print("="*70)
    print("Training ALS Model")
    print("="*70)

    # Load data
    print("\n[1/6] Loading data...")
    movies, ratings, links, tags = load_data()
    print(f"  ✓ Loaded {len(ratings)} ratings, {len(movies)} movies")

    # Build splits
    print("\n[2/6] Building train/val/test splits...")
    train_data, val_data, test_data = make_splits(ratings, thresh=4)
    print(f"  ✓ Train: {len(train_data):,} ratings")
    print(f"  ✓ Val:   {len(val_data):,} ratings")
    print(f"  ✓ Test:  {len(test_data):,} ratings")

    # Build index maps
    print("\n[3/6] Building index maps...")
    user_ids, movie_ids, user_index_map, movie_index_map = build_index_maps(ratings, train_data)
    print(f"  ✓ Users:  {len(user_ids):,}")
    print(f"  ✓ Movies: {len(movie_ids):,}")

    # Build matrix
    print("\n[4/6] Building user-item matrix...")
    X_users_items = train_matrix(train_data, user_ids, movie_ids,
                                  user_index_map, movie_index_map, alpha=20)
    print(f"  ✓ Shape: {X_users_items.shape}")
    density = 100 * X_users_items.nnz / (X_users_items.shape[0] * X_users_items.shape[1])
    print(f"  ✓ Density: {density:.2f}% ({X_users_items.nnz:,} non-zero entries)")

    # Train ALS
    print("\n[5/6] Training ALS (factors=64, reg=0.02, iters=20)...")
    als = AlternatingLeastSquares(
        factors=64,
        regularization=0.02,
        iterations=20,
        random_state=42
    )
    # Transpose once here: implicit expects items × users
    als.fit(X_users_items.T)
    print("  ✓ Training complete")

    # Evaluate
    print("\n[6/6] Evaluating on test set...")
    recall_10 = simple_recall_at_k(als, X_users_items, user_ids, movie_ids,
                                    user_index_map, test_data, k=10)
    print(f"  ✓ Recall@10: {recall_10:.4f}")

    # Save artifacts
    print("\nSaving artifacts...")
    np.savez_compressed(
        ART / "als.npz",
        user_factors=als.user_factors.astype(np.float32),
        item_factors=als.item_factors.astype(np.float32)
    )
    print("  ✓ als.npz")

    np.save(ART / "users.npy", user_ids)
    print("  ✓ users.npy")

    np.save(ART / "items.npy", movie_ids)
    print("  ✓ items.npy")

    sp.save_npz(ART / "X_users_items.npz", X_users_items)
    print("  ✓ X_users_items.npz")

    movies[["movieId", "title"]].to_csv(ART / "movie_map.csv", index=False)
    print("  ✓ movie_map.csv")

    print("\n" + "="*70)
    print("✓ Training complete! Artifacts saved to artifacts/")
    print("="*70)
    print(f"\nNext steps:")
    print(f"  1. Start API: uvicorn app.main:app --reload")
    print(f"  2. Test: curl 'http://127.0.0.1:8000/recommend?user_id=1&k=5'")
    print()


if __name__ == "__main__":
    main()
```

---

### **File: `app/recommender.py`**

```python
"""
Serving layer for ALS recommendations.
Loads pre-trained model artifacts and provides recommendation methods.
"""
from pathlib import Path
import numpy as np
import pandas as pd
import scipy.sparse as sp
from implicit.als import AlternatingLeastSquares

ART = Path("artifacts")


class ALSRecommender:
    """ALS-based movie recommender that loads pre-trained artifacts."""

    def __init__(self):
        """Load model artifacts and validate they exist."""
        self._validate_artifacts()

        # Load factors
        data = np.load(ART / "als.npz")
        self.user_f = data["user_factors"]
        self.item_f = data["item_factors"]

        # Load ID arrays
        self.users = np.load(ART / "users.npy")   # index → userId
        self.items = np.load(ART / "items.npy")   # index → movieId

        # Load movie titles
        movie_map = pd.read_csv(ART / "movie_map.csv")
        self.title_by_id = dict(zip(movie_map.movieId.astype(int), movie_map.title))

        # Reconstruct model object (just attach factors)
        self.model = AlternatingLeastSquares()
        self.model.user_factors = self.user_f
        self.model.item_factors = self.item_f

        # Load serving matrix (users × items)
        self.X_users_items = sp.load_npz(ART / "X_users_items.npz").tocsr()

        # Build ID → index lookup dicts
        self.uidx = {int(u): i for i, u in enumerate(self.users)}
        self.iidx = {int(m): i for i, m in enumerate(self.items)}

    def _validate_artifacts(self):
        """Check that all required artifacts exist."""
        if not ART.exists():
            raise FileNotFoundError(
                f"Artifacts directory '{ART}' not found. "
                "Run 'python train_als.py' first."
            )

        required = ["als.npz", "users.npy", "items.npy", "X_users_items.npz", "movie_map.csv"]
        missing = [f for f in required if not (ART / f).exists()]

        if missing:
            raise FileNotFoundError(
                f"Missing artifacts: {missing}. "
                "Run 'python train_als.py' first."
            )

    def recommend_known(self, user_id: int, k: int = 10):
        """
        Recommend movies for a known user.

        Args:
            user_id: User ID from training data
            k: Number of recommendations

        Returns:
            List of dicts with movieId and title
        """
        if user_id not in self.uidx:
            return []

        u = self.uidx[user_id]
        user_row = self.X_users_items[u:u+1]  # Extract user's interaction row

        # Get recommendations (filtering already-liked items)
        recs = self.model.recommend(
            u,
            user_row,
            N=k,
            filter_already_liked_items=True,
            recalculate_user=False
        )

        # Convert to movie IDs and titles
        mids = [int(self.items[i]) for i, _ in recs]
        return [{"movieId": m, "title": self.title_by_id.get(m, str(m))} for m in mids]

    def recommend_cold(self, liked_movie_ids=None, k: int = 10):
        """
        Recommend movies for cold-start (new user with seed likes).

        Args:
            liked_movie_ids: List of movie IDs the user likes
            k: Number of recommendations

        Returns:
            List of dicts with movieId and title
        """
        liked_movie_ids = liked_movie_ids or []

        # Build synthetic user row from seed movies
        row = sp.csr_matrix((1, len(self.items)), dtype=np.float32)

        if liked_movie_ids:
            # Map movie IDs to indices (skip unknown movies)
            cols = [self.iidx[m] for m in liked_movie_ids if m in self.iidx]

            if cols:
                # Use same confidence as training (1 + alpha = 21)
                data = np.full(len(cols), 21.0, dtype=np.float32)
                row = sp.csr_matrix(
                    (data, ([0] * len(cols), cols)),
                    shape=(1, len(self.items))
                )

        # Get recommendations (recalculate user embedding from seed)
        # Request extra to account for filtering
        recs = self.model.recommend(
            0,  # Dummy user index (not used when recalculate_user=True)
            row,
            N=k + len(liked_movie_ids),
            filter_already_liked_items=True,
            recalculate_user=True
        )

        # Manually filter seed movies (implicit's filter doesn't work perfectly for synthetic users)
        seed_set = set(liked_movie_ids)
        mids = [int(self.items[i]) for i, _ in recs if int(self.items[i]) not in seed_set][:k]

        return [{"movieId": m, "title": self.title_by_id.get(m, str(m))} for m in mids]
```

---

### **File: `app/main.py`**

```python
"""
FastAPI application for movie recommendations.

Endpoints:
    GET /health - Health check
    GET /recommend - Get recommendations (known user or cold-start)
"""
from fastapi import FastAPI, Query, HTTPException
from typing import Optional, List
from app.recommender import ALSRecommender

app = FastAPI(
    title="Movie Recommender API",
    description="ALS-based movie recommendations with cold-start support",
    version="1.0.0"
)

# Load model on startup
try:
    rec = ALSRecommender()
except FileNotFoundError as e:
    print(f"ERROR: {e}")
    print("Please run 'python train_als.py' first.")
    exit(1)


@app.get("/health")
def health():
    """Health check endpoint."""
    return {
        "status": "ok",
        "model": "ALS",
        "n_users": len(rec.users),
        "n_items": len(rec.items)
    }


@app.get("/recommend")
def recommend(
    user_id: Optional[int] = Query(None, description="Known user ID from training data"),
    liked: Optional[str] = Query(None, description="Comma-separated movie IDs for cold-start"),
    k: int = Query(10, ge=1, le=100, description="Number of recommendations")
):
    """
    Get movie recommendations.

    Two modes:
    1. Known user: Provide user_id
    2. Cold-start: Provide liked (comma-separated movie IDs)

    Examples:
        /recommend?user_id=1&k=5
        /recommend?liked=296,318,593&k=10
    """
    # Parse liked movies if provided
    liked_ids: List[int] = []
    if liked:
        try:
            liked_ids = [int(x.strip()) for x in liked.split(",") if x.strip()]
        except ValueError:
            raise HTTPException(
                status_code=400,
                detail="Invalid format for 'liked'. Expected comma-separated integers."
            )

    # Known-user path
    if user_id is not None:
        if user_id not in rec.uidx:
            raise HTTPException(
                status_code=404,
                detail=f"User {user_id} not found in training data. Use 'liked' parameter for cold-start."
            )

        recs = rec.recommend_known(user_id, k=k)
        return {
            "mode": "known-user",
            "user_id": user_id,
            "k": k,
            "recommendations": recs
        }

    # Cold-start path
    else:
        recs = rec.recommend_cold(liked_movie_ids=liked_ids, k=k)
        return {
            "mode": "cold-start",
            "seed": liked_ids,
            "k": k,
            "recommendations": recs
        }


@app.get("/movies/{movie_id}")
def get_movie(movie_id: int):
    """Get movie title by ID."""
    if movie_id in rec.title_by_id:
        return {
            "movieId": movie_id,
            "title": rec.title_by_id[movie_id]
        }
    raise HTTPException(status_code=404, detail="Movie not found")
```

---

### **File: `.gitignore` (additions)**

```gitignore
# Data (user downloads)
data_small/

# Generated artifacts
artifacts/*
!artifacts/.gitkeep

# Python
__pycache__/
*.pyc
.venv/
*.npz
*.npy

# IDE
.vscode/
.idea/
```

---

### **File: `README.md` (additions)**

Add this section to your existing README:

```markdown
## API Usage

### 1. Train the Model

```bash
python train_als.py
```

This generates artifacts in `artifacts/`:
- `als.npz` - User and item factors
- `users.npy`, `items.npy` - ID mappings
- `X_users_items.npz` - Serving matrix
- `movie_map.csv` - Movie titles

### 2. Start the API

```bash
uvicorn app.main:app --reload
```

API will be available at `http://127.0.0.1:8000`

### 3. API Endpoints

#### Health Check
```bash
curl http://127.0.0.1:8000/health
```

Response:
```json
{
  "status": "ok",
  "model": "ALS",
  "n_users": 610,
  "n_items": 9724
}
```

#### Known User Recommendations
```bash
curl "http://127.0.0.1:8000/recommend?user_id=1&k=5"
```

Response:
```json
{
  "mode": "known-user",
  "user_id": 1,
  "k": 5,
  "recommendations": [
    {"movieId": 318, "title": "Shawshank Redemption, The (1994)"},
    {"movieId": 858, "title": "Godfather, The (1972)"},
    ...
  ]
}
```

#### Cold-Start Recommendations (seed with likes)
```bash
curl "http://127.0.0.1:8000/recommend?liked=296,318,593&k=5"
```

Response:
```json
{
  "mode": "cold-start",
  "seed": [296, 318, 593],
  "k": 5,
  "recommendations": [
    {"movieId": 858, "title": "Godfather, The (1972)"},
    ...
  ]
}
```

## Project Structure

```
movie-recs/
├── app/
│   ├── __init__.py         # Package marker
│   ├── core.py             # Shared data/matrix functions
│   ├── recommender.py      # Serving layer
│   └── main.py             # FastAPI app
├── train_als.py            # Training script
├── artifacts/              # Generated model files
└── data_small/             # MovieLens data (user downloads)
```

## Development

### Running Locally

1. **Setup environment**:
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # or `.venv\Scripts\activate` on Windows
   pip install -r requirements.txt
   ```

2. **Download data**:
   - Get MovieLens `ml-latest-small` from [GroupLens](https://grouplens.org/datasets/movielens/latest/)
   - Extract to `data_small/`

3. **Train model**:
   ```bash
   python train_als.py
   ```

4. **Start API**:
   ```bash
   uvicorn app.main:app --reload
   ```

5. **Test in browser**:
   - Open `http://127.0.0.1:8000/docs` for interactive API docs
   - Try example requests
```

---

## Testing the Changes

### **Phase 1: Structure Setup (5 min)**

```bash
cd /path/to/movie-recs

# Create package
mkdir -p app
touch app/__init__.py

# Create artifacts directory
mkdir -p artifacts
touch artifacts/.gitkeep

# Verify structure
tree -L 2 -I '__pycache__|.venv|*.pyc'
```

Expected output:
```
.
├── app/
│   └── __init__.py
├── artifacts/
│   └── .gitkeep
├── data_small/
│   ├── movies.csv
│   ├── ratings.csv
│   ├── links.csv
│   └── tags.csv
└── train_als.py
```

---

### **Phase 2: Training (10 min)**

```bash
# Run training
python train_als.py
```

**Expected output**:
```
======================================================================
Training ALS Model
======================================================================

[1/6] Loading data...
  ✓ Loaded 100,836 ratings, 9,742 movies

[2/6] Building train/val/test splits...
  ✓ Train: 48,580 ratings
  ✓ Val:   305 ratings
  ✓ Test:  305 ratings

[3/6] Building index maps...
  ✓ Users:  610
  ✓ Movies: 9,724

[4/6] Building user-item matrix...
  ✓ Shape: (610, 9724)
  ✓ Density: 0.82% (48,580 non-zero entries)

[5/6] Training ALS (factors=64, reg=0.02, iters=20)...
  ✓ Training complete

[6/6] Evaluating on test set...
  ✓ Recall@10: 0.1246

Saving artifacts...
  ✓ als.npz
  ✓ users.npy
  ✓ items.npy
  ✓ X_users_items.npz
  ✓ movie_map.csv

======================================================================
✓ Training complete! Artifacts saved to artifacts/
======================================================================

Next steps:
  1. Start API: uvicorn app.main:app --reload
  2. Test: curl 'http://127.0.0.1:8000/recommend?user_id=1&k=5'
```

**Verify artifacts**:
```bash
ls -lh artifacts/

# Expected:
# als.npz           (~3-5 MB)
# users.npy         (~5 KB)
# items.npy         (~40 KB)
# X_users_items.npz (~1-2 MB)
# movie_map.csv     (~500 KB)
```

---

### **Phase 3: API Testing (10 min)**

**Start API**:
```bash
uvicorn app.main:app --reload
```

**Expected output**:
```
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process
INFO:     Started server process
INFO:     Waiting for application startup.
INFO:     Application startup complete.
```

**Test endpoints** (in another terminal):

1. **Health check**:
   ```bash
   curl http://127.0.0.1:8000/health
   ```
   Expected: `{"status":"ok","model":"ALS","n_users":610,"n_items":9724}`

2. **Known user**:
   ```bash
   curl "http://127.0.0.1:8000/recommend?user_id=1&k=5"
   ```
   Expected: JSON with 5 movie recommendations

3. **Cold-start**:
   ```bash
   curl "http://127.0.0.1:8000/recommend?liked=296,318&k=5"
   ```
   Expected: JSON with 5 recommendations based on seed movies

4. **Interactive docs**:
   - Open browser: `http://127.0.0.1:8000/docs`
   - Try out endpoints interactively

---

## Common Issues & Solutions

### **Issue 1: `ModuleNotFoundError: No module named 'app'`**

**Cause**: Running from wrong directory

**Solution**: Always run from repo root:
```bash
cd /path/to/movie-recs
python train_als.py  # ✓ Correct
```

Not from subdirectories:
```bash
cd app
python ../train_als.py  # ✗ Wrong
```

---

### **Issue 2: `FileNotFoundError: data_small/movies.csv`**

**Cause**: Data not downloaded or wrong working directory

**Solution**:
1. Download MovieLens `ml-latest-small` from GroupLens
2. Extract to `data_small/` in repo root
3. Verify: `ls data_small/` should show `movies.csv`, `ratings.csv`, etc.

---

### **Issue 3: API fails with `Missing artifacts`**

**Cause**: Haven't run training yet

**Solution**:
```bash
python train_als.py  # Generate artifacts first
uvicorn app.main:app --reload  # Then start API
```

---

### **Issue 4: `filter_already_liked_items` not working in cold-start**

**Cause**: Known limitation - implicit's filter doesn't work perfectly for synthetic users

**Solution**: Already handled in code - we manually filter seed movies after getting recommendations

---

## Success Criteria

✅ **Structure**: `app/` package exists with `__init__.py`, `core.py`, `recommender.py`, `main.py`

✅ **Training**: `python train_als.py` completes and saves 5 artifact files

✅ **Evaluation**: Recall@10 > 0.10 (expect ~0.12-0.13)

✅ **API**: All 3 endpoints return valid JSON responses

✅ **Known-user**: Recommendations don't include already-liked movies

✅ **Cold-start**: Recommendations don't include seed movies

✅ **Error handling**: Helpful errors when artifacts missing or user not found

---

## Timeline

- **Step 1**: Structure setup - 5 min
- **Step 2**: `app/core.py` - 10 min
- **Step 3**: `train_als.py` - 15 min
- **Step 4**: `app/recommender.py` - 20 min
- **Step 5**: `app/main.py` - 10 min
- **Step 6**: `.gitignore` - 2 min
- **Step 7**: `requirements.txt` - 2 min
- **Step 8**: `README.md` - 15 min
- **Testing**: 25 min

**Total: ~1.5 hours** (excluding training time, which is ~2-3 minutes)

---

## Next Steps After Implementation

1. **Test with different users**: Try user IDs 1-50 to verify diverse recommendations
2. **Test edge cases**: Non-existent user, invalid movie IDs, k=1, k=100
3. **Measure latency**: Time API responses (should be <100ms)
4. **Add logging**: Log requests and errors for debugging
5. **Add more endpoints**:
   - `GET /users/{user_id}/history` - Show user's past ratings
   - `POST /feedback` - Accept new ratings for online learning
   - `GET /similar/{movie_id}` - Find similar movies

---

## References

- [Implicit library docs](https://benfred.github.io/implicit/)
- [FastAPI docs](https://fastapi.tiangolo.com/)
- [MovieLens dataset](https://grouplens.org/datasets/movielens/)
- [Collaborative Filtering for Implicit Feedback Datasets (Hu et al. 2008)](http://yifanhu.net/PUB/cf.pdf)
